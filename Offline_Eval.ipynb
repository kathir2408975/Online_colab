{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Install Required Dependencies\n",
        "\n",
        "This cell installs all necessary Python packages for the evaluation pipeline\n",
        "dotenv: For loading environment variables\n",
        "\n",
        "Langsmith: LangSmith client for dataset management and experiment tracking\n",
        "\n",
        "requests: HTTP library for API calls\n",
        "\n",
        "deepeval: Framework for evaluating LLM outputs with various metrics\n",
        "\n",
        "openai, langchain, langchain-openai: For LLM interactions and chains\n",
        "\n",
        "langchain_community: Vector database and community integrations\n"
      ],
      "metadata": {
        "id": "ozb5OYnA7H_F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Am29VuVc7D1H"
      },
      "outputs": [],
      "source": [
        "%pip install dotenv langsmith\n",
        "%pip install requests\n",
        "%pip install deepeval==3.6.6\n",
        "%pip install openai langchain langchain-openai langchain_community"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Core Libraries\n",
        "\n",
        "Imports essential modules for:\n",
        "Environment variable management (dotenv)\n",
        "\n",
        "Azure OpenAI client initialization\n",
        "\n",
        "LangSmith client for experiment tracking and tracing\n",
        "\n",
        "Wrapper utilities to integrate OpenAI with LangSmith tracing\n"
      ],
      "metadata": {
        "id": "xDWZDqjM7LyF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "from openai import AzureOpenAI\n",
        "from langsmith import Client, traceable\n",
        "from langsmith.wrappers import wrap_openai"
      ],
      "metadata": {
        "id": "XA2Fsbdd7PaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set LangSmith API Key\n",
        "Configures the LangSmith API key as an environment variable\n",
        "\n",
        "This key is required for authenticating with LangSmith services\n",
        "\n",
        "Used for tracking experiments, storing datasets, and logging evaluation results\n"
      ],
      "metadata": {
        "id": "Lj0Al8uj7RUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"LANGSMITH_API_KEY\"]  = \"*********\"   ---->> Add you langsmith API Key"
      ],
      "metadata": {
        "id": "vMFbQM527UWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Initialize LangSmith Client\n",
        "Creates a LangSmith client instance using the API key from environment variables\n",
        "This client will be used to:\n",
        "Fetch datasets\n",
        "\n",
        "Create and manage experiments\n",
        "\n",
        "Log evaluation results\n",
        "\n",
        "Confirms successful initialization with a success message\n"
      ],
      "metadata": {
        "id": "4DwSe1Av7Wti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls = Client(api_key=os.environ[\"LANGSMITH_API_KEY\"])\n",
        "print(\"‚úÖ LangSmith client initialized.\")"
      ],
      "metadata": {
        "id": "0DtzWI6T7ZZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configure Azure OpenAI Environment\n",
        "Sets up environment variables for Azure OpenAI service:\n",
        "- API Key: Authentication credential for Azure OpenAI\n",
        "- API Base: Endpoint URL for your Azure OpenAI resource\n",
        "- API Version: Specifies the API version to use\n",
        "- Deployment Name: The specific GPT model deployment to use\n",
        "\n",
        "Then initializes the DeepEval Azure OpenAI model wrapper with:\n",
        "- Temperature=1: Controls randomness in model outputs\n",
        "- All Azure-specific configuration parameters\n",
        "This model will be used by DeepEval metrics for evaluation"
      ],
      "metadata": {
        "id": "XzXxj2jU7d_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Set environment variables (ensure names match what you will use later)\n",
        "os.environ[\"AZURE_OPENAI_API_KEY\"]        = \"**********\"  ---->> Add your Azure OpenAI API Key\n",
        "os.environ[\"AZURE_OPENAI_API_BASE\"]       = \"***********\" ----->> Add your Azure OpenAI API Base\n",
        "os.environ[\"AZURE_OPENAI_API_VERSION\"]    = \"***********\" ------->> Add your Azure OpenAI API Version\n",
        "os.environ[\"AZURE_OPENAI_LLM_DEPLOYMENT\"] = \"gpt-5-mini\"\n",
        "\n",
        "# Import the model class\n",
        "from deepeval.models import AzureOpenAIModel\n",
        "\n",
        "# Initialize the model\n",
        "azure_client = AzureOpenAIModel(\n",
        "    model_name=\"gpt-5-mini\",\n",
        "    deployment_name=os.environ[\"AZURE_OPENAI_LLM_DEPLOYMENT\"],\n",
        "    azure_openai_api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
        "    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
        "    azure_endpoint=os.environ[\"AZURE_OPENAI_API_BASE\"],\n",
        "    temperature=1\n",
        ")\n"
      ],
      "metadata": {
        "id": "Nevn-EQ87hAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fetch and Display Dataset from LangSmith\n",
        "Purpose: Verify dataset contents before running evaluation\n",
        "Steps:\n",
        "1. Connect to LangSmith using API key\n",
        "2. Fetch all examples from the specified dataset ID\n",
        "3. Loop through each example and display:\n",
        "    - Question: The input query   \n",
        "    -Actual Answer: The model's response\n",
        "   - Expected Answer: The ground truth/reference answer\n",
        "\n",
        "This is useful for:\n",
        "- Verifying dataset structure\n",
        "- Checking data quality before evaluation\n",
        "- Understanding the format of inputs and outputs"
      ],
      "metadata": {
        "id": "WGTnW2QM7jhK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Imports\n",
        "import os\n",
        "import pandas as pd\n",
        "from langsmith import Client\n",
        "from deepeval.test_case import LLMTestCase\n",
        "from deepeval.metrics import AnswerRelevancyMetric, SummarizationMetric, ToxicityMetric\n",
        "from tabulate import tabulate\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# -----------------------------\n",
        "# 1Ô∏è‚É£ Connect to LangSmith\n",
        "# -----------------------------\n",
        "client = Client(api_key=\"****************\") ----->> Add here langsmithe API key\n",
        "dataset_id = \"*****************\" -------->> Add here dataset ID\n",
        "\n",
        "# Fetch all examples\n",
        "examples = client.list_examples(dataset_id=dataset_id)\n",
        "\n",
        "# Loop through all examples and print their content\n",
        "for ex in examples:\n",
        "    question = ex.inputs.get(\"Question\", \"\")\n",
        "    actual_answer = ex.outputs.get(\"Actual_Answer\", \"\")\n",
        "    expected_answer = ex.outputs.get(\"Expected_Answer\", \"\")\n",
        "\n",
        "    print(\"Question:\", question)\n",
        "    print(\"Actual Answer:\", actual_answer)\n",
        "    print(\"Expected Answer:\", expected_answer)\n",
        "    print(\"-\" * 50)\n",
        "\n"
      ],
      "metadata": {
        "id": "cMMqbA687mj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run DeepEval Metrics and Save Results\n",
        "This is the MAIN EVALUATION CELL that performs the complete evaluation pipeline\n",
        "Process:\n",
        "CONNECT TO LANGSMITH\n",
        "- Initializes client and fetches dataset examples\n",
        "- Prepares evaluation cases with input, actual output, and expected output\n",
        "SELECT METRICS\n",
        "- AnswerRelevancyMetric: Measures how relevant the answer is to the question\n",
        " -SummarizationMetric: Evaluates quality of summarization\n",
        " - ToxicityMetric: Detects toxic or harmful content\n",
        "\n",
        "DEFINE EVALUATION FUNCTION\n",
        "Creates LLMTestCase for each example\n",
        "Runs all metrics on each test case\n",
        "Captures scores and reasoning for each metric\n",
        " Displays results in HTML table format\n",
        "\n",
        "EVALUATE ALL EXAMPLES\n",
        "- Loops through each example in the dataset\n",
        "- Runs evaluate_case() for each\n",
        "- Collects all results in all_results_list\n",
        "\n",
        "SAVE RESULTS TO CSV\n",
        " - Converts results to DataFrame\n",
        "- Saves to CSV file for later use\n",
        "- Each row contains: Question, Metric, Score, Reason, Actual_Answer, Expected_Answer\n"
      ],
      "metadata": {
        "id": "7CxvfZ5T7pHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Imports\n",
        "import os\n",
        "import pandas as pd\n",
        "from langsmith import Client\n",
        "from deepeval.test_case import LLMTestCase\n",
        "from deepeval.metrics import AnswerRelevancyMetric, SummarizationMetric, ToxicityMetric\n",
        "from tabulate import tabulate\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# -----------------------------\n",
        "# 1Ô∏è‚É£ Connect to LangSmith\n",
        "# -----------------------------\n",
        "client = Client(api_key=\"****************\") ----->> Add here langsmithe API key\n",
        "dataset_id = \"*****************\" -------->> Add here dataset ID\n",
        "\n",
        "\n",
        "# Fetch all examples\n",
        "examples = client.list_examples(dataset_id=dataset_id)\n",
        "\n",
        "\n",
        "# Prepare evaluation cases\n",
        "eval_cases = [\n",
        "    {\n",
        "        \"input\": ex.inputs.get(\"Question\", \"\"),\n",
        "        \"actual_output\": ex.outputs.get(\"Actual_Answer\", \"\"),\n",
        "        \"expected_output\": ex.outputs.get(\"Expected_Answer\", \"\")\n",
        "    }\n",
        "    for ex in examples\n",
        "]\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 2Ô∏è‚É£ Pick metrics\n",
        "# -----------------------------\n",
        "metrics_list = [AnswerRelevancyMetric, SummarizationMetric, ToxicityMetric]\n",
        "\n",
        "# -----------------------------\n",
        "# 3Ô∏è‚É£ Evaluate function\n",
        "# -----------------------------\n",
        "def evaluate_case(case_data, model=None, metrics_list=None):\n",
        "    if metrics_list is None or model is None:\n",
        "        return\n",
        "\n",
        "    test_case = LLMTestCase(\n",
        "        input=case_data[\"input\"],\n",
        "        expected_output=case_data[\"expected_output\"],\n",
        "        actual_output=case_data[\"actual_output\"],\n",
        "        retrieval_context=None,\n",
        "        context=None\n",
        "    )\n",
        "\n",
        "    all_results = []\n",
        "    for metric in metrics_list:\n",
        "        try:\n",
        "            m = metric(model=model)\n",
        "            m.measure(test_case)\n",
        "            all_results.append({\n",
        "                \"Metric\": type(m).__name__,\n",
        "                \"Score\": m.score,\n",
        "                \"Reason\": getattr(m, \"reason\", None)\n",
        "            })\n",
        "        except Exception as e:\n",
        "            all_results.append({\n",
        "                \"Metric\": type(metric).__name__,\n",
        "                \"Score\": None,\n",
        "                \"Reason\": f\"Error: {e}\"\n",
        "            })\n",
        "\n",
        "    print(f\"\\nüìò Results for: {case_data['input']}\\n\")\n",
        "    display(HTML(tabulate(\n",
        "        [[r[\"Metric\"], r[\"Score\"], r[\"Reason\"]] for r in all_results],\n",
        "        headers=[\"Metric\", \"Score\", \"Reason\"],\n",
        "        tablefmt=\"html\"\n",
        "    )))\n",
        "\n",
        "    return all_results\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 4Ô∏è‚É£ Evaluate all examples\n",
        "# -----------------------------\n",
        "all_results_list = []\n",
        "\n",
        "for case in eval_cases:\n",
        "    results = evaluate_case(case, model=azure_client, metrics_list=metrics_list)\n",
        "    for r in results:\n",
        "        all_results_list.append({\n",
        "            \"Question\": case[\"input\"],\n",
        "            \"Expected_Answer\": case[\"expected_output\"],\n",
        "            \"Actual_Answer\": case[\"actual_output\"],\n",
        "            \"Metric\": r[\"Metric\"],\n",
        "            \"Score\": r[\"Score\"],\n",
        "            \"Reason\": r[\"Reason\"]\n",
        "        })\n",
        "\n",
        "# -----------------------------\n",
        "# 5Ô∏è‚É£ Save results to CSV (optional)\n",
        "# -----------------------------\n",
        "df_results = pd.DataFrame(all_results_list)\n",
        "df_results.to_csv(\"deepeval_results_updated_final.csv\", index=False)\n",
        "print(\"‚úÖ Results saved to deepeval_results.csv\")\n"
      ],
      "metadata": {
        "id": "2Tpcx24o7o0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload Evaluation Results to LangSmith UI\n",
        "This cell uploads the saved DeepEval results to LangSmith for visualization\n",
        "Purpose: Display all evaluation metrics in the LangSmith web interface\n",
        "deepeval_wrapper(): Takes inputs (question) and returns outputs (answer + metrics)\n",
        "Looks up results from CSV for each question\n",
        "Returns formatted output with all metric scores and reasons\n",
        "\n",
        "DEFINE EVALUATOR FORMATTERS\n",
        "- answer_relevancy_evaluator: Extracts AnswerRelevancyMetric score\n",
        "- summarization_evaluator: Extracts SummarizationMetric score\n",
        "- toxicity_evaluator: Extracts ToxicityMetric score\n",
        "- Each evaluator returns: key (metric name), score (float), comment (reason)\n",
        "\n",
        " RUN EVALUATION FORMATTER & UPLOAD\n",
        "- Uses ls_client.evaluate() to create experiment in LangSmith\n",
        "- Processes each example with the wrapper function\n",
        "- Applies all three evaluators to capture scores\n",
        "- Uploads results to LangSmith dashboard\n",
        "Result: All metrics visible in LangSmith UI with scores and explanations\n"
      ],
      "metadata": {
        "id": "bzKhnG_t7ubc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Imports\n",
        "import os\n",
        "import pandas as pd\n",
        "from langsmith import Client\n",
        "from datetime import datetime\n",
        "\n",
        "# -----------------------------\n",
        "# 1Ô∏è‚É£ Connect to LangSmith\n",
        "# -----------------------------\n",
        "client = Client(api_key=\"****************\") ----->> Add here langsmithe API key\n",
        "dataset_id = \"*****************\" -------->> Add here dataset ID\n",
        "dataset = ls_client.read_dataset(dataset_id=dataset_id)\n",
        "print(f\"‚úÖ Using existing dataset: {dataset.name} (ID: {dataset.id})\")\n",
        "\n",
        "# -----------------------------\n",
        "# 3Ô∏è‚É£ Load DeepEval results from CSV\n",
        "# -----------------------------\n",
        "df_results = pd.read_csv(\"/content/deepeval_results_updated_final.csv\")\n",
        "print(f\"‚úÖ Loaded {len(df_results)} results from CSV\")\n",
        "print(f\"üìä Unique questions: {df_results['Question'].nunique()}\")\n",
        "\n",
        "# -----------------------------\n",
        "# 4Ô∏è‚É£ Prepare model wrapper function\n",
        "# -----------------------------\n",
        "def Resultslogback_wrapper(inputs: dict) -> dict:\n",
        "    \"\"\"\n",
        "    Returns actual answer and all metrics for a given question.\n",
        "    \"\"\"\n",
        "    question = inputs.get(\"Question\", \"\")\n",
        "    rows = df_results[df_results[\"Question\"] == question]\n",
        "\n",
        "    if rows.empty:\n",
        "        return {\"answer\": None, \"metrics\": {}}\n",
        "\n",
        "    actual_answer = rows.iloc[0][\"Actual_Answer\"]\n",
        "    metrics = {}\n",
        "    for _, row in rows.iterrows():\n",
        "        metrics[row[\"Metric\"]] = {\"Score\": row[\"Score\"], \"Reason\": row[\"Reason\"]}\n",
        "\n",
        "    return {\"answer\": actual_answer, \"metrics\": metrics}\n",
        "\n",
        "# -----------------------------\n",
        "# 5Ô∏è‚É£ Dynamic evaluator for all metrics\n",
        "# -----------------------------\n",
        "def dynamic_metrics_evaluator(run, example):\n",
        "    outputs = run.outputs\n",
        "    metrics = outputs.get(\"metrics\", {})\n",
        "\n",
        "    results = []\n",
        "    for metric_name, data in metrics.items():\n",
        "        results.append({\n",
        "            \"key\": metric_name.lower(),\n",
        "            \"score\": float(data.get(\"Score\", 0.0)),\n",
        "            \"comment\": data.get(\"Reason\", \"\")\n",
        "        })\n",
        "    return results\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 6Ô∏è‚É£ Run evaluation and upload to LangSmith\n",
        "# -----------------------------\n",
        "experiment_name = f\"deepeval-metrics-{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "print(f\"\\nüì§ Starting LangSmith evaluation: {experiment_name}\\n\")\n",
        "\n",
        "results = ls_client.evaluate(\n",
        "    Resultslogback_wrapper,      # Model wrapper that returns answer + all metrics\n",
        "    data=dataset_id,             # Dataset ID\n",
        "    evaluators=[dynamic_metrics_evaluator],\n",
        "    experiment_prefix=experiment_name,\n",
        "    description=\"DeepEval metrics uploaded dynamically\",\n",
        "    max_concurrency=1,\n",
        "    num_repetitions=1,\n",
        ")\n",
        "\n",
        "print(\"\\n‚úÖ Evaluation complete!\")\n",
        "print(f\"üìä Results object: {results}\")\n"
      ],
      "metadata": {
        "id": "YxiJyE167zLl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}