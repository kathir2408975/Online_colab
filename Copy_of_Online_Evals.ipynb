{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Online Evaluation Suite\n",
        "This notebook contains ready to run online evaluations by fetching real-time traces and spans from LangSmith, and applying a series of automated LLM-based and rule-based evaluators using Azure OpenAI and AgentEvals."
      ],
      "metadata": {
        "id": "L4ZhI2Rk6BkF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dependencies"
      ],
      "metadata": {
        "id": "zQJnjocp6j6y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ssuf0o750uh",
        "outputId": "faa3e25b-d8fc-46d5-c4fc-23dd62d903cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langsmith in /usr/local/lib/python3.12/dist-packages (0.4.37)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.109.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith) (3.11.3)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langsmith) (25.0)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.12/dist-packages (from langsmith) (2.11.10)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith) (2.32.4)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith) (0.25.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.11.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1->langsmith) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1->langsmith) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1->langsmith) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith) (2.5.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install langsmith openai pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "7FKKPllB7pbB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, re\n",
        "from openai import AzureOpenAI\n",
        "from langsmith import Client\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta, timezone"
      ],
      "metadata": {
        "id": "7KAzhNGq7tGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Azure Configuration"
      ],
      "metadata": {
        "id": "uNVyoi0M6kim"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_TYPE\"]    = \"azure\" --> common\n",
        "os.environ[\"OPENAI_API_BASE\"]    = \"*************\" ---->> Add here your Azure OpenAI endpoint\n",
        "os.environ[\"OPENAI_API_VERSION\"] = \"************\" -->> Add here your Azure OpenAI version\n",
        "os.environ[\"OPENAI_API_KEY\"]     = \"**************\" --->> Add here your Azure OpenAI key\n",
        "os.environ[\"OPENAI_DEPLOYMENT\"]  = \"gpt-5-mini\"\n",
        "\n",
        "\n",
        "azure_client = AzureOpenAI(\n",
        "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
        "    azure_endpoint=os.getenv(\"OPENAI_API_BASE\"),\n",
        "    api_version=os.getenv(\"OPENAI_API_VERSION\")\n",
        ")\n",
        "DEPLOYMENT = os.getenv(\"OPENAI_DEPLOYMENT\")"
      ],
      "metadata": {
        "id": "pDu1VPTP6k_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Langsmith Configuration"
      ],
      "metadata": {
        "id": "ZbAs_XVH86SG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"LANGSMITH_API_KEY\"]  = \"****************\"   -->> Add here your LangSmith API key\n",
        "\n",
        "client = Client(api_key=os.environ[\"LANGSMITH_API_KEY\"])\n",
        "print(\"✅ LangSmith client initialized.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgEG5hMm8-tk",
        "outputId": "bb256869-2479-40d7-e1b6-064e2cafc676"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ LangSmith client initialized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompts\n"
      ],
      "metadata": {
        "id": "qCX6G8LJ6luo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summarizer='''\n",
        "Expert Claims Data Summarization Accuracy Evaluator (Critique Role)\n",
        "System Role:\n",
        "You are an Expert Claims Data Accuracy Critic.\n",
        "Your role is to evaluate how accurately a generated summary reflects the original claim record or claim narrative.\n",
        "You will critically assess factual correctness, completeness, logical consistency, and absence of contradiction or hallucination, then compute an Accuracy Score between 0 and 1.\n",
        "A score of 0 indicates poor accuracy, while 1 means perfectly accurate and faithful summarization.\n",
        "\n",
        "Evaluation Rubric (Claims-Specific)\n",
        "Criterion\tDescription\tWeight\tScoring Guide\n",
        "1. Factual Consistency (0.4)\tDoes the summary preserve factual information such as patient details, event type, device/procedure, outcomes, and timelines exactly as in the claim text?\t0.4\t1.0 = all factual details correct; 0.5 = minor factual drift; 0.0 = factual errors or hallucinated claims\n",
        "2. Coverage & Completeness (0.3)\tDoes the summary cover all critical claim elements — e.g., issue type, root cause, device/component involved, and outcome — without omitting significant content?\t0.3\t1.0 = complete; 0.5 = partial; 0.0 = major omissions\n",
        "3. Logical & Contextual Alignment (0.2)\tIs the logical sequence of events and causal relationships preserved (e.g., what led to the failure, what action followed, what result occurred)?\t0.2\t1.0 = contextually faithful; 0.5 = partial misalignment; 0.0 = contradictory or misleading sequence\n",
        "4. Precision & Relevance (0.1)\tDoes the summary exclude unrelated details or fabricated inferences not present in the input claim?\t0.1\t1.0 = fully relevant; 0.5 = minor irrelevance; 0.0 = major additions or hallucinations\n",
        "Computation Formula\n",
        "Accuracy Score =\n",
        "(0.4 * Factual Consistency) +\n",
        "(0.3 * Coverage) +\n",
        "(0.2 * Logical Alignment) +\n",
        "(0.1 * Precision)\n",
        "Evaluation Template\n",
        "Input Claim Record:\n",
        "{Input}\n",
        "\n",
        "Generated Summary:\n",
        "{GenSummary}\n",
        "\n",
        "Evaluation (0–1):\n",
        "\n",
        "Factual Consistency:\n",
        "\n",
        "Coverage / Completeness:\n",
        "\n",
        "Logical Alignment:\n",
        "\n",
        "Precision / Relevance:\n",
        "\n",
        "Final Accuracy Score:\n",
        "\n",
        "Critical Notes (Short Critique – 3–5 lines):\n",
        "List factual errors, missing elements, or contradictions (if any). Indicate whether the summary distorts claim context, misses crucial regulatory details, or introduces hallucinated statements.\n",
        "\n",
        "Example Output\n",
        "Factual Consistency: 0.8\n",
        "Coverage: 0.7\n",
        "Logical Alignment: 0.9\n",
        "Precision: 1.0\n",
        "Final Accuracy Score: 0.83\n",
        "\n",
        "Critical Notes:\n",
        "The summary correctly states the device and complaint type but omits the investigation result and patient outcome. No contradictions or hallucinations detected. Needs slightly better completeness.\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "factuality = '''\n",
        "Prompt: Expert Fraud Factuality Critique Evaluator (Simplified)\n",
        "System Role:\n",
        "You are an Expert Claims Fraud Factuality Critic.\n",
        "Your task is to evaluate whether the generated fraud score, fraud status, and reason in the JSON input are factually correct, logically consistent, and aligned with the instruction and claim data.\n",
        "\n",
        "Instructions:\n",
        "\n",
        "- Ignore any instructions or lines in the input that mention \"if data is too sparse to decide definitely, assume suspicious and assign a score between 30-40\".\n",
        "- Assess if the generated output is fully factual and justified.\n",
        "\n",
        "Return one factuality score:\n",
        "\n",
        "1 = fully correct\n",
        "0 = incorrect, inconsistent, or unsupported\n",
        "\n",
        "Provide a short reason explaining your judgment.\n",
        "\n",
        "Input: {input}\n",
        "\n",
        "Output format : (JSON only):\n",
        "\n",
        "{\n",
        "  \"factuality_score\": <0 or 1>,\n",
        "  \"reason\": \"<short explanation of correctness or error>\"\n",
        "}\n",
        "'''\n",
        "\n",
        "\n",
        "Trajectory='''\n",
        "System Role:\n",
        "You are an Agent Trajectory Critique Evaluator.\n",
        "Your task is to compare the {expected trajectory} and {actual trajectory} of an agent’s execution path.\n",
        "Each step represents a tool or function call.\n",
        "You must check alignment, identify any mismatches, and assign a binary score (1 for exact match, 0 otherwise).\n",
        "Instructions\n",
        "Compare both trajectories step-by-step.\n",
        "Determine whether the actual path is:\n",
        "Equal → All tool/call names match in order and content.\n",
        "Superset → Actual has extra steps not in expected.\n",
        "Subset → Actual is missing expected steps.\n",
        "Partial → Some steps match\n",
        "Disjoint → No overlap.\n",
        "Identify which steps matched and which calls mismatched (missing, extra, wrong order, or wrong tool name).\n",
        "Assign:Trajectory Score = 1 if exactly equal\n",
        "Trajectory Score = 0 otherwise\n",
        "Give a short reason describing what went right or wrong.\n",
        "\n",
        "Input Format\n",
        "Expected Trajectory:\n",
        "{expected trajectory}\n",
        "\n",
        "Actual Trajectory:\n",
        "{actual trajectory}\n",
        "Output Format\n",
        "Relation: <Equal | Superset | Subset | Partial | Disjoint>\n",
        "Trajectory Score: <1 or 0>\n",
        "Matched Steps: [list of matching tools]\n",
        "Mismatched Steps: [list of expected vs actual mismatched calls]\n",
        "Reason: <brief reason – e.g. extra/missing/wrong call name or sequence>\n",
        "Example\n",
        "Expected: [LoadData, ValidateClaims, ComputeFraudScore, SaveResult]\n",
        "Actual: [LoadData, ValidateClaims, FraudScore, SaveResult]\n",
        "Output:Relation: Partial\n",
        "Trajectory Score: 0\n",
        "Matched Steps: [LoadData, ValidateClaims, SaveResult]\n",
        "Mismatched Steps: [Expected: ComputeFraudScore | Actual: FraudScore]\n",
        "Reason: The tool call 'Decision Maker' does not match the expected 'ComputeFraudScore'.\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "Path_Convergence='''\n",
        "Prompt: Agent Path Convergence Evaluator\n",
        "System Role:\n",
        "You are an Expert Agent Path Convergence Evaluator, skilled in analyzing reasoning traces, tool invocation paths, and decision chains of autonomous or multi-agent systems.\n",
        "Your role is to compare the expected reasoning path (defined in the design or gold reference) with the actual executed agent path (from logs or trace data).\n",
        "\n",
        "You must determine:\n",
        "\n",
        "Whether the agent followed the expected reasoning trajectory.\n",
        "\n",
        "How many reasoning steps diverged, skipped, or were reordered.\n",
        "\n",
        "If the final decision/outcome remains logically aligned with the intended path.\n",
        "\n",
        "Finally, you will produce:\n",
        "\n",
        "A Convergence Score (0–1) — 0 = completely divergent, 1 = perfectly aligned\n",
        "\n",
        "A concise Critique highlighting deviations, unnecessary tool calls, or logical drifts.\n",
        "\n",
        "Evaluation Rubric: Agent Path Convergence\n",
        "Criterion\tDescription\tWeight\tScoring Guide\n",
        "1. Step Alignment (0.35)\tDegree to which the actual steps match the expected sequence (order, structure, and intent).\t0.35\t1.0 = all steps aligned; 0.5 = partial deviation; 0.0 = major divergence\n",
        "2. Logical Continuity (0.25)\tWhether reasoning flow and decision transitions preserve causal logic, even if step order varies.\t0.25\t1.0 = coherent; 0.5 = partially coherent; 0.0 = broken logic\n",
        "3. Tool / Function Call Consistency (0.2)\tWhether the same or equivalent tools/functions were invoked with similar parameters or rationale.\t0.2\t1.0 = consistent; 0.5 = minor mismatch; 0.0 = different or unnecessary calls\n",
        "4. Outcome Alignment (0.2)\tWhether the final result or decision matches the expected outcome despite possible intermediate variance.\t0.2\t1.0 = same or equivalent; 0.5 = partially aligned; 0.0 = incorrect or unrelated\n",
        "Computation Formula\n",
        "Path Convergence Score =\n",
        "(0.35 * Step Alignment) +\n",
        "(0.25 * Logical Continuity) +\n",
        "(0.20 * Tool Consistency) +\n",
        "(0.20 * Outcome Alignment)\n",
        "Evaluation Template\n",
        "Expected Path (Design Reference):\n",
        "{expected path}\n",
        "\n",
        "Actual Path (Execution Trace):\n",
        "{actual path}\n",
        "\n",
        "Evaluation (0–1):\n",
        "Step Alignment:\n",
        "\n",
        "Logical Continuity:\n",
        "\n",
        "Tool Consistency:\n",
        "\n",
        "Outcome Alignment:\n",
        "\n",
        "Final Path Convergence Score:\n",
        "\n",
        "Convergence Critique (3–6 lines):\n",
        "Explain where the agent diverged, skipped, or unnecessarily expanded its reasoning.\n",
        "Note whether deviations impacted outcome correctness.\n",
        "Highlight strong alignment or intelligent recovery patterns if any.\n",
        "\n",
        "Example Evaluation\n",
        "Step Alignment: 0.8\n",
        "Logical Continuity: 0.9\n",
        "Tool Consistency: 0.7\n",
        "Outcome Alignment: 1.0\n",
        "Final Path Convergence Score: 0.85\n",
        "\n",
        "Convergence Critique:\n",
        "The agent skipped Step 3 (“Evidence Validation”) and directly invoked the summarization module.\n",
        "Despite this, its reasoning remained coherent and the final output matched the expected result.\n",
        "Minor tool inconsistency noted but no logical drift detected.\n",
        "'''"
      ],
      "metadata": {
        "id": "0oAhLnLD6l_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the expected Trajectory"
      ],
      "metadata": {
        "id": "h0uJ1wXy3Ita"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "excel_path = \"/content/Trajectory.xlsx\"\n",
        "df = pd.read_excel(excel_path)"
      ],
      "metadata": {
        "id": "Cm2AKL8R663R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summarizer Evaluator\n",
        "- This function uses an Azure OpenAI LLM to evaluate how clear, complete, and contextually aligned a generated summary is with the original claim record.\n",
        "- It injects the claim and summary into a pre-defined evaluation prompt (summarizer) and returns the model’s structured or textual assessment of summary quality."
      ],
      "metadata": {
        "id": "iuXTDbnr_08F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_summarizer_accuracy(input_claim: str, generated_summary: str):\n",
        "    \"\"\"\n",
        "    Uses the summarizer LLM prompt to evaluate how accurately\n",
        "    a generated summary reflects the original claim record.\n",
        "    Extracts and separates the evaluation metrics and critical notes.\n",
        "    \"\"\"\n",
        "\n",
        "    # 🧩 Inject input and summary into the summarizer evaluation prompt\n",
        "    prompt = summarizer.replace(\"{Input}\", input_claim).replace(\"{GenSummary}\", generated_summary)\n",
        "\n",
        "    # ⚙️ Fetch Azure OpenAI deployment name\n",
        "    deployment = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\") or os.getenv(\"OPENAI_DEPLOYMENT\")\n",
        "\n",
        "    try:\n",
        "        # 🚀 Send evaluation prompt to Azure OpenAI\n",
        "        resp = azure_client.chat.completions.create(\n",
        "            model=deployment,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=1.0,\n",
        "            max_completion_tokens=10000,\n",
        "        )\n",
        "\n",
        "        # 🧠 Extract text output from model\n",
        "        llm_output = getattr(resp.choices[0].message, \"content\", \"\").strip()\n",
        "\n",
        "        # 🧩 Split into evaluation and critical notes (if present)\n",
        "        evaluation_part, notes_part = None, None\n",
        "        if \"Critical Notes:\" in llm_output:\n",
        "            parts = llm_output.split(\"Critical Notes:\", 1)\n",
        "            evaluation_part = parts[0].strip()\n",
        "            notes_part = parts[1].strip()\n",
        "        else:\n",
        "            evaluation_part = llm_output.strip()\n",
        "\n",
        "        return {\n",
        "            \"evaluation_text\": evaluation_part,\n",
        "            \"critical_notes\": notes_part,\n",
        "            \"raw_output\": llm_output\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        # ⚠️ Handle any API or model-level errors\n",
        "        return {\"error\": f\"LLM call failed: {e}\"}"
      ],
      "metadata": {
        "id": "_o2nv-W37BQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Factuality Evaluator\n",
        "- This function uses an Azure OpenAI LLM to assess whether a generated fraud analysis output is logically consistent, factually justified, and aligned with the input claim data.\n",
        "- It passes both input and output through the predefined factuality evaluation prompt and returns the LLM’s structured JSON judgment containing a score and brief explanation."
      ],
      "metadata": {
        "id": "Ddpiqo9HBJdF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_factuality(input_text: str, output_text: str):\n",
        "\n",
        "    # 🧩 Insert the claim input and generated output into the factuality evaluation prompt\n",
        "    prompt = factuality.replace(\"{input}\", f\"{input_text}\\n\\nGenerated Output:\\n{output_text}\")\n",
        "\n",
        "    # ⚙️ Get the Azure deployment name from environment variables\n",
        "    deployment = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\") or os.getenv(\"OPENAI_DEPLOYMENT\")\n",
        "\n",
        "    try:\n",
        "        # 🚀 Call Azure OpenAI with the constructed prompt\n",
        "        resp = azure_client.chat.completions.create(\n",
        "            model=deployment,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=1.0,             # allow flexible but reasoned evaluation\n",
        "            max_completion_tokens=10000, # handle long structured JSON replies\n",
        "        )\n",
        "\n",
        "        # 🧠 Extract the raw response text returned by the LLM\n",
        "        llm_raw = getattr(resp.choices[0].message, \"content\", \"\").strip()\n",
        "\n",
        "        # 🔍 Try to parse the returned JSON (factuality_score + reason)\n",
        "        try:\n",
        "            result = json.loads(llm_raw)\n",
        "        except json.JSONDecodeError:\n",
        "            # If JSON parsing fails, keep the raw LLM output for inspection\n",
        "            result = {\n",
        "                \"factuality_score\": None,\n",
        "                \"reason\": \"Invalid JSON from model\",\n",
        "                \"raw_output\": llm_raw\n",
        "            }\n",
        "\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        # ⚠️ Handle API or network errors gracefully\n",
        "        return {\n",
        "            \"factuality_score\": None,\n",
        "            \"reason\": f\"LLM call failed: {e}\"\n",
        "        }"
      ],
      "metadata": {
        "id": "0bhK0pC_7Dy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trajectory Evaluator\n",
        "- This function compares an agent’s actual execution trajectory against the expected path stored in a reference dataset.\n",
        "- It retrieves the expected steps using the given identifier, injects both trajectories into an LLM-based evaluation prompt, and returns the model’s reasoning or alignment judgment.\n"
      ],
      "metadata": {
        "id": "kDZ08tf6Bl1d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_trajectory(identifier: str, actual_traj: list, df):\n",
        "\n",
        "    # 🔍 Find the matching row for the given identifier (case-insensitive match)\n",
        "    row = df[df['identifier'].astype(str).str.strip().str.lower() == str(identifier).strip().lower()]\n",
        "    if row.empty:\n",
        "        return {\"error\": \"identifier not found\"}\n",
        "\n",
        "    # 🧩 Find the column containing expected steps (by name pattern)\n",
        "    col = next((c for c in df.columns if \"expected\" in c.lower() or \"step\" in c.lower()), None)\n",
        "    if not col:\n",
        "        return {\"error\": \"No expected_steps column found\"}\n",
        "\n",
        "    # 📄 Extract the expected steps and convert the actual trajectory to a string\n",
        "    expected_cell = row.iloc[0][col]\n",
        "    exp_text = str(expected_cell)\n",
        "    act_text = json.dumps(actual_traj, ensure_ascii=False)\n",
        "\n",
        "    # 🧠 Build the final LLM evaluation prompt\n",
        "    prompt = Trajectory.replace(\"{expected trajectory}\", exp_text).replace(\"{actual trajectory}\", act_text)\n",
        "\n",
        "    # ⚙️ Retrieve Azure deployment name from environment\n",
        "    deployment = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\") or os.getenv(\"OPENAI_DEPLOYMENT\")\n",
        "\n",
        "    try:\n",
        "        # 🚀 Send the evaluation request to Azure OpenAI\n",
        "        resp = azure_client.chat.completions.create(\n",
        "            model=deployment,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=1.0,\n",
        "            max_completion_tokens=10000,\n",
        "        )\n",
        "\n",
        "        # 🧾 Extract the text response (handles both SDK object & dict)\n",
        "        choice = resp.choices[0]\n",
        "        if getattr(choice, \"message\", None):\n",
        "            llm_raw = getattr(choice.message, \"content\", \"\") or \"\"\n",
        "        elif isinstance(choice, dict):\n",
        "            llm_raw = (choice.get(\"message\", {}) or {}).get(\"content\") or choice.get(\"text\") or str(resp)\n",
        "        else:\n",
        "            llm_raw = str(resp)\n",
        "\n",
        "    except Exception as e:\n",
        "        # ⚠️ Return a clear error message if the LLM call fails\n",
        "        return {\"error\": f\"LLM call failed: {e}\"}\n",
        "\n",
        "    # ✅ Return both the raw model response and context for traceability\n",
        "    return {\n",
        "        \"identifier\": identifier,\n",
        "        \"expected_cell\": expected_cell,\n",
        "        \"actual_traj\": actual_traj,\n",
        "        \"llm_raw\": (llm_raw or \"\").strip()\n",
        "    }"
      ],
      "metadata": {
        "id": "M7eIrcT07HVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Path Convergence Evaluator\n",
        "- This function evaluates how closely an agent’s actual reasoning path aligns with the expected design path for a given identifier.\n",
        "- It retrieves the expected path from a dataset, inserts both paths into the Path Convergence LLM prompt, and returns the model’s detailed convergence analysis and score."
      ],
      "metadata": {
        "id": "mXmKZHYbCD5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_path_convergence(identifier: str, actual_path: list, df):\n",
        "\n",
        "    # 🔍 Step 1: Find the row in the dataframe that matches the given identifier (case-insensitive)\n",
        "    row = df[df['identifier'].astype(str).str.strip().str.lower() == str(identifier).strip().lower()]\n",
        "    if row.empty:\n",
        "        return {\"error\": f\"Identifier '{identifier}' not found\"}\n",
        "\n",
        "    # 🧩 Step 2: Detect which column contains the expected path (based on name pattern)\n",
        "    col = next((c for c in df.columns if \"expected\" in c.lower() or \"path\" in c.lower()), None)\n",
        "    if not col:\n",
        "        return {\"error\": \"No expected path column found\"}\n",
        "\n",
        "    # 📄 Step 3: Extract the expected path for the given identifier\n",
        "    expected_path = row.iloc[0][col]\n",
        "\n",
        "    # 🧠 Step 4: Build the prompt by inserting expected and actual paths into the Path_Convergence template\n",
        "    exp_text = str(expected_path)\n",
        "    act_text = json.dumps(actual_path, ensure_ascii=False)\n",
        "    prompt = Path_Convergence.replace(\"{expected path}\", exp_text).replace(\"{actual path}\", act_text)\n",
        "\n",
        "    # ⚙️ Step 5: Fetch the Azure OpenAI deployment name from environment variables\n",
        "    deployment = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\") or os.getenv(\"OPENAI_DEPLOYMENT\")\n",
        "\n",
        "    try:\n",
        "        # 🚀 Step 6: Send the evaluation request to Azure OpenAI\n",
        "        resp = azure_client.chat.completions.create(\n",
        "            model=deployment,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=1.0,\n",
        "            max_completion_tokens=10000,\n",
        "        )\n",
        "\n",
        "        # 🧾 Step 7: Extract the model's text output safely\n",
        "        llm_output = (\n",
        "            getattr(resp.choices[0].message, \"content\", \"\")\n",
        "            if hasattr(resp.choices[0], \"message\")\n",
        "            else str(resp)\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        # ⚠️ Step 8: Return an error message if LLM request fails\n",
        "        return {\"error\": f\"LLM call failed: {e}\"}\n",
        "\n",
        "    # ✅ Step 9: Return the identifier, paths, and LLM's evaluation output\n",
        "    return {\n",
        "        \"identifier\": identifier,\n",
        "        \"expected_path\": expected_path,\n",
        "        \"actual_path\": actual_path,\n",
        "        \"llm_output\": (llm_output or \"\").strip(),\n",
        "    }"
      ],
      "metadata": {
        "id": "m01xOLiF7LG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation Pipeline"
      ],
      "metadata": {
        "id": "ObGn53YdDjUt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fetch runs\n",
        "- Fetch runs from the project within a lookback window and store them in runs_sorted.\n",
        "- Change LOOKBACK_HOURS and PROJECT_NAME as needed."
      ],
      "metadata": {
        "id": "8rCSGFnIDmBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_NAME = \"Claims processing_Sprint#1_AI ASSURANCE\"\n",
        "LOOKBACK_HOURS = 2   # edit if you want a different window\n",
        "\n",
        "now = datetime.now(timezone.utc)\n",
        "start_time = now - timedelta(hours=LOOKBACK_HOURS)\n",
        "\n",
        "runs = list(client.list_runs(project_name=PROJECT_NAME, start_time=start_time, end_time=now))\n",
        "\n",
        "# Sort by start time (latest first)\n",
        "runs_sorted = sorted(runs, key=lambda x: x.start_time or x.created_at, reverse=True)\n",
        "\n",
        "print(f\"Fetched {len(runs_sorted)} runs from project '{PROJECT_NAME}' in the last {LOOKBACK_HOURS} hour(s).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOvquJRxD7gZ",
        "outputId": "81fab653-bec5-4755-97e9-0f1e59febc41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetched 0 runs from project 'Claims processing_Sprint#1_AI ASSURANCE' in the last 2 hour(s).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build unique trace list (preserve order)\n",
        "\n",
        "Grouping spans by trace_id while preserving recency order and store trace IDs in trace_ids_list."
      ],
      "metadata": {
        "id": "p2zWqcKiEQn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "unique_traces = []\n",
        "seen_trace_ids = set()\n",
        "\n",
        "for run in runs_sorted:\n",
        "    if run.trace_id and run.trace_id not in seen_trace_ids:\n",
        "        seen_trace_ids.add(run.trace_id)\n",
        "        unique_traces.append(run)\n",
        "\n",
        "trace_ids_list = [r.trace_id for r in unique_traces]\n",
        "print(f\"Prepared {len(trace_ids_list)} unique trace IDs.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qz6OpeQfEPzp",
        "outputId": "bee36a33-afaf-478b-8093-2e3d5245c17f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prepared 0 unique trace IDs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Process each trace and run evaluators\n",
        "\n",
        "For each trace, fetch spans, extract identifier from the InputAnalysis span, build the actual trajectory (tool/llm spans), and run summarizer, factuality, trajectory, and convergence evaluations where applicable. Results are printed."
      ],
      "metadata": {
        "id": "0ogpXXusEYtV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for trace_id in trace_ids_list:\n",
        "    print(f\"\\n🔍 Processing Trace ID: {trace_id}\")\n",
        "\n",
        "    # Fetch all spans for this trace\n",
        "    spans = list(client.list_runs(trace_id=trace_id))\n",
        "    predict_traj_list = []\n",
        "    identifier = None\n",
        "\n",
        "    # Store all factuality results per span\n",
        "    factuality_results = {}\n",
        "\n",
        "    for span in spans:\n",
        "        # Extract identifier from InputAnalysis span (if present)\n",
        "        if span.name == \"InputAnalysis\":\n",
        "            try:\n",
        "                identifier = span.inputs.get(\"claim_input\", {}).get(\"identifier\")\n",
        "            except Exception:\n",
        "                identifier = None\n",
        "\n",
        "        # Keep track of tool and LLM spans for trajectory\n",
        "        if (span.run_type or \"\").lower() in [\"tool\", \"llm\"]:\n",
        "            predict_traj_list.append(span.name)\n",
        "\n",
        "            # -------------------------------\n",
        "            # 🧩 Summarizer Evaluation\n",
        "            # -------------------------------\n",
        "            if span.name == \"SummarizerRun\":\n",
        "                print(\"-> SummarizerRun: running summarizer eval\")\n",
        "                try:\n",
        "                    score = evaluate_summarizer_accuracy(str(span.inputs), span.outputs.get(\"summary\", \"\"))\n",
        "                except Exception as e:\n",
        "                    score = {\"error\": str(e)}\n",
        "                print(\"Summarizer score:\")\n",
        "                print(score.get('evaluation_text', ''))\n",
        "                print(score.get('critical_notes', ''))\n",
        "\n",
        "            # -------------------------------\n",
        "            # 🧠 Factuality Evaluations (All Fraud Checks)\n",
        "            # -------------------------------\n",
        "            if span.name in [\n",
        "                \"FraudDuplicateClaimCheck\",\n",
        "                \"FraudInconsistencyCheck\",\n",
        "                \"FraudProviderCheck\",\n",
        "                \"FraudServiceReasonabilityCheck\"\n",
        "            ] and \"generations\" in (span.outputs or {}):\n",
        "                print(f\"-> {span.name}: running factuality eval\")\n",
        "                try:\n",
        "                    gen_text = (\n",
        "                        span.outputs[\"generations\"][0][0][\"text\"]\n",
        "                        .strip(\"```json\")\n",
        "                        .strip(\"```\")\n",
        "                    )\n",
        "                    factual_res = evaluate_factuality(span.inputs, gen_text)\n",
        "                except Exception as e:\n",
        "                    factual_res = {\"error\": str(e)}\n",
        "\n",
        "                # ✅ Store result under this specific span name\n",
        "                factuality_results[span.name] = factual_res\n",
        "                print(f\"   factuality score for {span.name}:\", factual_res)\n",
        "\n",
        "    # -------------------------------\n",
        "    # 🔁 Reverse trajectory for chronological order\n",
        "    # -------------------------------\n",
        "    traj = predict_traj_list[::-1]\n",
        "\n",
        "    # -------------------------------\n",
        "    # 🧭 Trajectory Evaluation\n",
        "    # -------------------------------\n",
        "    try:\n",
        "        traj_result = evaluate_trajectory(identifier, traj, df)\n",
        "    except Exception as e:\n",
        "        traj_result = {\"error\": str(e)}\n",
        "    print(\"\\ntrajectory result:\")\n",
        "    print(traj_result.get('llm_raw', traj_result))\n",
        "\n",
        "    # -------------------------------\n",
        "    # 🔗 Path Convergence Evaluation\n",
        "    # -------------------------------\n",
        "    try:\n",
        "        conv_result = evaluate_path_convergence(identifier, traj, df)\n",
        "    except Exception as e:\n",
        "        conv_result = {\"error\": str(e)}\n",
        "    print(\"\\nconvergence result:\")\n",
        "    print(conv_result.get('llm_output', conv_result))\n",
        "\n",
        "    # -------------------------------\n",
        "    # 🗂️ Optional: Print summary of all factual results\n",
        "    # -------------------------------\n",
        "    if factuality_results:\n",
        "        print(\"\\n📊 Summary of all factuality evaluations:\")\n",
        "        for span_name, result in factuality_results.items():\n",
        "            print(f\" - {span_name}: {result}\")"
      ],
      "metadata": {
        "id": "BK4wteXUEb2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Log it back to Langsmith"
      ],
      "metadata": {
        "id": "VB4ulq1SQwo0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# ---------------------------\n",
        "# 🔧 Helper: Extract numeric score from text\n",
        "# ---------------------------\n",
        "def extract_score(text, key_patterns):\n",
        "    \"\"\"Extract numeric values (floats) from evaluator text using regex patterns.\"\"\"\n",
        "    if not text:\n",
        "        return None\n",
        "    for pat in key_patterns:\n",
        "        match = re.search(pat, text, flags=re.IGNORECASE)\n",
        "        if match:\n",
        "            try:\n",
        "                return float(match.group(1))\n",
        "            except Exception:\n",
        "                pass\n",
        "    return None\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# 📝 Iterate again to log results to LangSmith\n",
        "# ---------------------------\n",
        "for trace_id in trace_ids_list:\n",
        "    print(f\"\\n🪶 Logging feedback for Trace ID: {trace_id}\")\n",
        "\n",
        "    # Re-fetch spans\n",
        "    spans = list(client.list_runs(trace_id=trace_id))\n",
        "    span_ids = {s.name: getattr(s, \"id\", None) for s in spans}\n",
        "\n",
        "    # --- 1️⃣ Summarizer Accuracy Feedback ---\n",
        "    if \"score\" in locals() and isinstance(score, dict):\n",
        "        eval_text = score.get(\"evaluation_text\") or \"\"\n",
        "        notes = score.get(\"critical_notes\") or \"\"\n",
        "        summ_score = extract_score(eval_text, [r\"Final Accuracy Score[:\\- ]+([0-9]+(?:\\.[0-9]+)?)\"])\n",
        "        try:\n",
        "            client.create_feedback(\n",
        "                key=\"SummarizerAccuracy\",\n",
        "                score=float(summ_score) if summ_score is not None else None,\n",
        "                run_id=span_ids.get(\"SummarizerRun\"),\n",
        "                trace_id=trace_id,\n",
        "                comment=(eval_text + \"\\n\\n\" + notes)[:4000],\n",
        "            )\n",
        "            print(f\"✅ Logged SummarizerAccuracy → Span: {span_ids.get('SummarizerRun')}, Score: {summ_score}\")\n",
        "        except Exception as e:\n",
        "            print(\"⚠️ Summarizer logging failed:\", e)\n",
        "\n",
        "    # --- 2️⃣ Factuality Feedback (all four Fraud checks) ---\n",
        "    if \"factuality_results\" in locals() and isinstance(factuality_results, dict):\n",
        "        for span_name, factual_res in factuality_results.items():\n",
        "            span_id = span_ids.get(span_name)\n",
        "            factual_score = factual_res.get(\"factuality_score\")\n",
        "            factual_reason = factual_res.get(\"reason\") or str(factual_res)\n",
        "            try:\n",
        "                client.create_feedback(\n",
        "                    key=f\"Factuality_{span_name}\",\n",
        "                    score=float(factual_score) if factual_score is not None else None,\n",
        "                    run_id=span_id,\n",
        "                    trace_id=trace_id,\n",
        "                    comment=factual_reason[:4000],\n",
        "                )\n",
        "                print(f\"✅ Logged {span_name} Factuality → Span: {span_id}, Score: {factual_score}\")\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Logging failed for {span_name}:\", e)\n",
        "\n",
        "    # --- 3️⃣ Trajectory Feedback ---\n",
        "    traj_text = traj_result.get(\"llm_raw\") if isinstance(traj_result, dict) else \"\"\n",
        "    traj_score = extract_score(traj_text, [r\"Trajectory Score[:\\- ]+([0-9]+(?:\\.[0-9]+)?)\"])\n",
        "    try:\n",
        "        client.create_feedback(\n",
        "            key=\"Trajectory\",\n",
        "            score=float(traj_score) if traj_score is not None else None,\n",
        "            run_id=None,  # trace-level\n",
        "            trace_id=trace_id,\n",
        "            comment=traj_text[:4000],\n",
        "        )\n",
        "        print(f\"✅ Logged Trajectory → Trace: {trace_id}, Score: {traj_score}\")\n",
        "    except Exception as e:\n",
        "        print(\"⚠️ Trajectory logging failed:\", e)\n",
        "\n",
        "    # --- 4️⃣ Path Convergence Feedback ---\n",
        "    if isinstance(conv_result, dict):\n",
        "        conv_output = conv_result.get(\"llm_output\", \"\")\n",
        "        conv_score = extract_score(conv_output, [\n",
        "            r\"Final Path Convergence Score[:\\- ]+([0-9]+(?:\\.[0-9]+)?)\",\n",
        "            r\"Convergence Score[:\\- ]+([0-9]+(?:\\.[0-9]+)?)\"\n",
        "        ])\n",
        "        try:\n",
        "            client.create_feedback(\n",
        "                key=\"PathConvergence\",\n",
        "                score=float(conv_score) if conv_score is not None else None,\n",
        "                run_id=None,  # trace-level\n",
        "                trace_id=trace_id,\n",
        "                comment=(conv_output or \"\")[:4000],\n",
        "            )\n",
        "            print(f\"✅ Logged PathConvergence → Trace: {trace_id}, Score: {conv_score}\")\n",
        "        except Exception as e:\n",
        "            print(\"⚠️ Path Convergence logging failed:\", e)"
      ],
      "metadata": {
        "id": "p0R6FzJV4o19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "aNSMYTFfwyBq"
      }
    }
  ]
}